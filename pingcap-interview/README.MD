
## 问题
100GB url 文件，使用 1GB 内存计算出出现次数 top100的 url和出现的次数
___

思路
使用map-reduece思想
### map阶段
 + 计算大文件要被分成多少个小文件，创建小文件
 + 逐行读入大文件数据，预聚合后存放到hashmap中，同时计算内存占用；
 + 内存占用达到上限后，将hashmap中的内容按hash分片，保证相同的url都被hash到相同的文件中，写出的格式 : key value；
 + 重复上述过程指导文件读取分割完成；
 + 检查所有分片文件大小都是否满足要求，如果不满足，递归的对子文件进行分割，保证最终所有的分片文件都满足要求
 + 内存占用=hashmap.size()*[length(url) + length(markword) + length(referance)+ length(hashcode) + length(long)] , url的长度 + 对象头长度+ url引用长度+ hashcode长度+出现次数（long）的长度

### reduce阶段
按如下步骤操作小文件，
+ 将一个小文件中的内容读取到hashmap中，这里会对相同的key进行聚合；
+ 将hashmap中的数据放入到容量为100小顶堆中，构造top100的url记录,循环的对每个小文件都进行同样的操作，直到所有小文件读取完毕，该小顶堆中的元素就是top100的数据；
+ 小顶堆使用Java自带的优先队列封装而成；


### 优化点
+ 文件输入输出使用缓冲流
+ 对于url超高基数情况，可以使用生产者消费者模型进行文件读写；
+ 对于较低基数，内存能放下所有聚合后的记录，不用进行文件拆分；
    
